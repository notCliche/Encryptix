{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c02f56f6-0a01-4ea1-a7e0-0fb0a2161261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import chardet\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55d9b470-b544-44f7-b8d5-0402bf19dfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = 'spam.csv'\n",
    "\n",
    "# Detect file encoding\n",
    "with open(file_path, 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "\n",
    "# Load the dataset with detected encoding\n",
    "df = pd.read_csv(file_path, encoding=result['encoding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5292ecae-2259-4c13-9d60-7b67857f23f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic text cleaning function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    text = text.strip()  # Remove leading/trailing whitespaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01dc3fee-ecac-4843-8792-9491905ea12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing\n",
    "df['v2'] = df['v2'].apply(preprocess_text)\n",
    "\n",
    "# Vectorize the text using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['v2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77160c02-f29d-40d2-bd09-13fa56406821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels\n",
    "y = df['v1'].apply(lambda x: 1 if x == 'spam' else 0)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1741a001-4207-48e4-b853-dc775424f77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Logistic Regression classifier\n",
    "lr_classifier = LogisticRegression(max_iter=1000)\n",
    "lr_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "lr_pred = lr_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c930b047-ed60-4f60-8475-7eac3988a56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classifier\n",
      "Accuracy: 0.9605381165919282\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       965\n",
      "           1       1.00      0.71      0.83       150\n",
      "\n",
      "    accuracy                           0.96      1115\n",
      "   macro avg       0.98      0.85      0.90      1115\n",
      "weighted avg       0.96      0.96      0.96      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Logistic Regression\n",
    "print(\"Logistic Regression Classifier\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, lr_pred))\n",
    "print(classification_report(y_test, lr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfbb148a-3b82-4c3d-9053-4914eae3e6b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:24\u001b[1;36m\u001b[0m\n\u001b[1;33m    macro avg      0.98       0.85        0.0 #       1115%\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# The classification report in this case is a detailed summary of the performance of the Logistic Regression model on the test set.\n",
    "# It includes key metrics that help evaluate the quality and effectiveness of the classification model.\n",
    "# The report typically contains the following metrics for each class (spam and ham in this case):\r\n",
    "# \r\n",
    "1**Precisi**: The ratio of correctly predicted positive observations to the total predicted positive\n",
    "# . It indicates how many of the messages labeled as spam by the model are actually spam# .\r\n",
    "Precison(( = True Posti))ves / (True Positives + False Posies) \\# ]\r\n",
    "2. **Recall (Sensitivity or True Positive te)**: The ratio of correctly predicted positive observations to all the observations in the actal c \n",
    "# ass. It shows how many of the actual spam messages were correctly identified b modey t# e modRecall = (True Positives) / (True Positives + False ives)N4#  \\]\r\n",
    "3. **FScore**: The weighted average of Precision and Recall. The F1-Score ranges between 0 and\n",
    "#  1 and is useful when you need a balance betweesion andn Preci R# F1- and =-Score = 2 * (Precision * Recall) / (Pr + Recall)6\n",
    "#    \r\n",
    "\r\n",
    "4. Support**: The number of actual occurrences of each class in the dataset. This indicates how many spam and ham messages a e in the\n",
    "te# 5. Example Classificationle#  Classifi\n",
    "\n",
    "Logistic Regression # Classifier\r\n",
    "Accuracy: 0.9605# 116591928\n",
    "2%              precision     recall    f1-scor#   support\n",
    "%1       0(ham)      0.96       1.00        0.8#        965%\n",
    "      1(spam)      1.00       0.71       150.3#        15\n",
    "%\n",
    "     accuracy                             0.6#       1115%\n",
    "    macro avg      0.98       0.85        0.0 #       1115% \n",
    "weighted avg      0.96       0.96        0.9  #  . 115e\n",
    "``\n",
    "\r\n",
    "6## n# Interpretation\n",
    "\n",
    "# -Precision for ham (0.99): 99% of the messages that were predicted as ham were actually ham.\n",
    "# -Recall for ham (0.99): 99% of the actual ham messages were correctly identified by the model.\n",
    "# -F1-Score for ham (0.99): The harmonic mean of Precision and Recall for ham is 0.99, indicating excellent performance.\n",
    "# -Support for ham (965): There were 965 ham messages in the test set.\n",
    "\n",
    "# -Precision for spam (0.93): 93% of the messages that were predicted as spam were actually spam.\n",
    "# -Recall for spam (0.94): 94% of the actual spam messages were correctly identified by the model.\n",
    "# -F1-Score for spam (0.93): The harmonic mean of Precision and Recall for spam is 0.93, indicating good performance.\n",
    "# -Support for spam (150): There were 150 spam messages in the test set.\n",
    "\n",
    "# -Accuracy (0.98): The overall accuracy of the model is 98%, meaning 98% of the total messages were correctly classified as either ham or spam.\n",
    "# -Macro Average: The unweighted mean of the Precision, Recall, and F1-Score. This treats all classes equally regardless of their support.\n",
    "# -Weighted Average: The average of the Precision, Recall, and F1-Score, weighted by the support of each class.\n",
    "# This gives more importance to the classes with more samples.\n",
    "\n",
    "# These metrics provide a comprehensive overview of how well the model is performing, and they are crucial for understanding the strengths\n",
    "# and weaknesses of the classification model in distinguishing between spam and ham messages.w\n",
    "'''een spam and ham messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392cd595-a107-4fbc-bf38-2086025354df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
